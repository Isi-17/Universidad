{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Robot localization\n",
    "\n",
    "In the upcoming chapters we will cover three fundamental aspects of mobile robots: robot localization, map building, and simultaneous localization and mapping (SLAM). \n",
    "\n",
    "Citing Cox [1]:\n",
    "\n",
    "> *Using sensory information to locate the robot in its environment is the most fundamental problem to providing a mobile robot with autonomous capabilities*\n",
    "\n",
    "Concretely, the goal of **robot localization** is, given a map of the environment and a sequence of sensor measurements, to retrieve the robot's pose in such environment. \n",
    "\n",
    "Localization problems can be grouped into different types (see Fig.1 below):\n",
    "- **Position tracking**: the robot knows approximately where it is (for example, using odometry readings, which have to come at a certain frequency).\n",
    "- **Global localization**: the robot has no clue where it is (*e.g.* GPS). It is needed when the robot is turned on.\n",
    "- **Kidnapped robot problem**: the robot thinks that it knows where it is, but is wrong!\n",
    "\n",
    "$\\\\[5pt]$\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/localization_problems_low.png\" alt=\"\">\n",
    "  <figcaption>Fig. 1. Different types of localization problems.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">OPTIONAL</span>\n",
    "\n",
    "<span style=\"color:green\">Surf the internet looking for more general information about robot localization. You can include additional definitions, examples, images, videos,... anything you find interesting!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simultaneous Localization and Mapping (SLAM) is a groundbreaking technology in robotics, addressing the challenge of enabling a robot to navigate and create a map of an unknown environment in real-time. The core idea behind SLAM is the concurrent estimation of the robot's position (localization) and the construction of a map of its surroundings.\n",
    "\n",
    "SLAM operates on the principle of integrating sensor data, such as laser scans, camera images, or depth information, to simultaneously update the robot's belief about its own location and construct a representation of the environment. This process involves handling uncertainties in sensor measurements and the robot's motion.\n",
    "\n",
    "One of the common methods employed in SLAM is the use of probabilistic approaches like particle filters or extended Kalman filters. These methods help manage the uncertainty associated with the robot's movement and sensor measurements, allowing for accurate and reliable localization and mapping.\n",
    "\n",
    "SLAM uses these sensors to build the environment map.\n",
    "Lidar SLAM: Uses laser range finders to measure distances to objects in the environment. Lidar-based SLAM is common in various applications, including autonomous vehicles.\n",
    "Visual SLAM: Utilizes cameras to extract visual features from the environment. Visual SLAM is crucial in scenarios where lidar data might be sparse or unavailable.\n",
    "RGB-D SLAM: Combines color information with depth data, often obtained from sensors like Microsoft Kinect or Intel RealSense cameras.\n",
    "\n",
    "It can be used in autonomous vehicles and drones to navigate and understand their surroundings.\n",
    "\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping\n",
    "\n",
    "Paper: FastSLAM: A Factored Solution to the Simultaneous Localization and Mapping Problem (Michael Montemerlo, Sebastian Thrun, Daphne Koller, Ben Wegbreit) http://robots.stanford.edu/papers/montemerlo.fastslam-tr.pdf\n",
    "\n",
    "Youtube: SLAM Course - Lecture 1 | Autonomous Navigation for Flying Robots (Cyrill Stachniss) https://www.youtube.com/watch?v=U6vr3iNrwRA&ab_channel=CyrillStachniss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">***END OF OPTIONAL PART***</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "[1] Cox, Ingemar J. \"Blanche-an experiment in guidance and navigation of an autonomous robot vehicle.\" IEEE Transactions on robotics and automation 7, no. 2 (1991): 193-204."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
